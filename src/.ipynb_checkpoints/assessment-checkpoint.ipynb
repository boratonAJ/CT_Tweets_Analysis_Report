{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cape Town, South Africa Analysis of Twitter posts\n",
    "\n",
    "#### Ajayi Olabode\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This is a simple program that analysis the Cape Town, South Africa tweets posts. We grouped the data based on location and the program then use the location to find the most frequency words users used on Twitter. Furthermore, we can used the analysis to find users ID and we predict the different emotions attached to the text. However, in this report we consider using the `user id`,`location`,`text`. \n",
    "\n",
    "#### Used Python Libraries\n",
    "This program data was pre-processed using pandas, NLTK, gensim, and numpy libraries within the Python virtual environment. Plots were created using plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named emoticons",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-71d0294c4e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0memoticons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmoticonDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named emoticons"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/python\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "import gensim\n",
    "\n",
    "# plotly configuration\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"location\", \"text\"])\n",
    "\n",
    "        else:\n",
    "            self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"text\"],dtype={\"id\":\"int64\",\"text\":\"str\"},nrows=4000)\n",
    "            not_null_text = 1 ^ pd.isnull(self.data[\"text\"])\n",
    "            not_null_id = 1 ^ pd.isnull(self.data[\"id\"])\n",
    "            self.data = self.data.loc[not_null_id & not_null_text, :]\n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT @ZackieAchmat: #UitmetZille #UitMetZuma - a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT @ZackieAchmat: #UitmetZille #UitMetZuma - a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT @ReclaimCT: Tonight @ReclaimCT came for sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT @ReclaimCT: Tonight @ReclaimCT came for sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>cape town</td>\n",
       "      <td>The demographics in that room (beyond Reclaim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>cape town</td>\n",
       "      <td>The demographics in that room (beyond Reclaim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>@ReclaimCT ...refer the part \\rI prefer to liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>@ReclaimCT ...refer the part \\rI prefer to liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT @ReclaimCT: Tonight @ReclaimCT came for sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT @ReclaimCT: Tonight @ReclaimCT came for sup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                 location  \\\n",
       "0  8.550000e+17             South Africa   \n",
       "1  8.550000e+17             South Africa   \n",
       "2  8.550000e+17             South Africa   \n",
       "3  8.550000e+17             South Africa   \n",
       "4  8.550000e+17                cape town   \n",
       "5  8.550000e+17                cape town   \n",
       "6  8.550000e+17  Cape Town, South Africa   \n",
       "7  8.550000e+17  Cape Town, South Africa   \n",
       "8  8.550000e+17  Cape Town, South Africa   \n",
       "9  8.550000e+17  Cape Town, South Africa   \n",
       "\n",
       "                                                text  \n",
       "0  RT @ZackieAchmat: #UitmetZille #UitMetZuma - a...  \n",
       "1  RT @ZackieAchmat: #UitmetZille #UitMetZuma - a...  \n",
       "2  RT @ReclaimCT: Tonight @ReclaimCT came for sup...  \n",
       "3  RT @ReclaimCT: Tonight @ReclaimCT came for sup...  \n",
       "4  The demographics in that room (beyond Reclaim ...  \n",
       "5  The demographics in that room (beyond Reclaim ...  \n",
       "6  @ReclaimCT ...refer the part \\rI prefer to liv...  \n",
       "7  @ReclaimCT ...refer the part \\rI prefer to liv...  \n",
       "8  RT @ReclaimCT: Tonight @ReclaimCT came for sup...  \n",
       "9  RT @ReclaimCT: Tonight @ReclaimCT came for sup...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"/Users/boratonaj/Desktop/Data_Science/Assessment/CT_Tweets_Analysis_Report/data/subset.csv\")\n",
    "data.processed_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TwitterCleanuper:\n",
    "    def iterate(self):\n",
    "        for cleanup_method in [self.remove_urls,\n",
    "                               self.remove_usernames,\n",
    "                               self.remove_na,\n",
    "                               self.remove_special_chars,\n",
    "                               self.remove_numbers]:\n",
    "            yield cleanup_method\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    def remove_by_regex(tweets, regexp):\n",
    "        tweets.loc[:, \"text\"].replace(regexp, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_urls(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_na(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"Not Available\"]\n",
    "\n",
    "    def remove_special_chars(self, tweets):  # it unrolls the hashtags to normal words\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                     \"!\", \"?\", \".\", \"'\",\n",
    "                                                                     \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "\n",
    "    def remove_usernames(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "    def remove_numbers(self, tweets):\n",
    "        return TwitterCleanuper.remove_by_regex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Cleansing(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "    def cleanup(self, cleanuper):\n",
    "        t = self.processed_data\n",
    "        for cleanup_method in cleanuper.iterate():\n",
    "            if not self.is_testing:\n",
    "                t = cleanup_method(t)\n",
    "            else:\n",
    "                if cleanup_method.__name__ != \"remove_na\":\n",
    "                    t = cleanup_method(t)\n",
    "\n",
    "        self.processed_data = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT UitmetZille UitMetZuma  as speaks to an all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT UitmetZille UitMetZuma  as speaks to an all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>cape town</td>\n",
       "      <td>The demographics in that room beyond Reclaim T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>cape town</td>\n",
       "      <td>The demographics in that room beyond Reclaim T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>refer the part \\rI prefer to live where I like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>refer the part \\rI prefer to live where I like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>RT Tonight came for supper with We done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>I think the actions of Reclaim the City amp fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>I think the actions of Reclaim the City amp fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Western Cape, South Africa</td>\n",
       "      <td>RT I think the actions of Reclaim the City amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Western Cape, South Africa</td>\n",
       "      <td>RT I think the actions of Reclaim the City amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>RT I think the actions of Reclaim the City amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town, South Africa</td>\n",
       "      <td>Last night went for supper with done with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Capetown, South Africa</td>\n",
       "      <td>RT Dat was een beste klim Maar het uitzicht ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>CPT SA Defending the underdog_ɥ</td>\n",
       "      <td>RT I think the actions of Reclaim the City amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>CPT SA Defending the underdog_ɥ</td>\n",
       "      <td>RT I think the actions of Reclaim the City amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town /\\/��������\\/\\, RSA</td>\n",
       "      <td>RT Dat was een beste klim Maar het uitzicht ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>cape town</td>\n",
       "      <td>RT Last night went for supper with done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Soweto, South Africa</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Soweto, South Africa</td>\n",
       "      <td>RT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town / New Orleans</td>\n",
       "      <td>RT Last night went for supper with done with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>RT Dat was een beste klim Maar het uitzicht ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Green Point, Cape Town</td>\n",
       "      <td>Tafelberg sale shows DA doesnt occupy moral hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Green Point, Cape Town</td>\n",
       "      <td>Impossible not to interpret actions and langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.550000e+17</td>\n",
       "      <td>Vanderbijlpark, South Africa</td>\n",
       "      <td>RT Cissie Gool House this morning _�ɍ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                         location  \\\n",
       "0   8.550000e+17                     South Africa   \n",
       "1   8.550000e+17                     South Africa   \n",
       "2   8.550000e+17                     South Africa   \n",
       "3   8.550000e+17                     South Africa   \n",
       "4   8.550000e+17                        cape town   \n",
       "5   8.550000e+17                        cape town   \n",
       "6   8.550000e+17          Cape Town, South Africa   \n",
       "7   8.550000e+17          Cape Town, South Africa   \n",
       "8   8.550000e+17          Cape Town, South Africa   \n",
       "9   8.550000e+17          Cape Town, South Africa   \n",
       "10  8.550000e+17          Cape Town, South Africa   \n",
       "11  8.550000e+17          Cape Town, South Africa   \n",
       "12  8.550000e+17                     South Africa   \n",
       "13  8.550000e+17                     South Africa   \n",
       "14  8.550000e+17       Western Cape, South Africa   \n",
       "15  8.550000e+17       Western Cape, South Africa   \n",
       "16  8.550000e+17                        Cape Town   \n",
       "17  8.550000e+17          Cape Town, South Africa   \n",
       "18  8.550000e+17           Capetown, South Africa   \n",
       "19  8.550000e+17  CPT SA Defending the underdog_ɥ   \n",
       "20  8.550000e+17  CPT SA Defending the underdog_ɥ   \n",
       "21  8.550000e+17    Cape Town /\\/��������\\/\\, RSA   \n",
       "22  8.550000e+17                        cape town   \n",
       "23  8.550000e+17             Soweto, South Africa   \n",
       "24  8.550000e+17             Soweto, South Africa   \n",
       "25  8.550000e+17         Cape Town / New Orleans    \n",
       "26  8.550000e+17                        Cape Town   \n",
       "27  8.550000e+17           Green Point, Cape Town   \n",
       "28  8.550000e+17           Green Point, Cape Town   \n",
       "29  8.550000e+17     Vanderbijlpark, South Africa   \n",
       "\n",
       "                                                 text  \n",
       "0   RT UitmetZille UitMetZuma  as speaks to an all...  \n",
       "1   RT UitmetZille UitMetZuma  as speaks to an all...  \n",
       "2   RT Tonight came for supper with We done with t...  \n",
       "3   RT Tonight came for supper with We done with t...  \n",
       "4   The demographics in that room beyond Reclaim T...  \n",
       "5   The demographics in that room beyond Reclaim T...  \n",
       "6     refer the part \\rI prefer to live where I like   \n",
       "7     refer the part \\rI prefer to live where I like   \n",
       "8   RT Tonight came for supper with We done with t...  \n",
       "9   RT Tonight came for supper with We done with t...  \n",
       "10  RT Tonight came for supper with We done with t...  \n",
       "11  RT Tonight came for supper with We done with t...  \n",
       "12  I think the actions of Reclaim the City amp fe...  \n",
       "13  I think the actions of Reclaim the City amp fe...  \n",
       "14  RT I think the actions of Reclaim the City amp...  \n",
       "15  RT I think the actions of Reclaim the City amp...  \n",
       "16  RT I think the actions of Reclaim the City amp...  \n",
       "17  Last night went for supper with done with the ...  \n",
       "18  RT Dat was een beste klim Maar het uitzicht ma...  \n",
       "19  RT I think the actions of Reclaim the City amp...  \n",
       "20  RT I think the actions of Reclaim the City amp...  \n",
       "21  RT Dat was een beste klim Maar het uitzicht ma...  \n",
       "22  RT Last night went for supper with done with t...  \n",
       "23                                                RT   \n",
       "24                                                RT   \n",
       "25  RT Last night went for supper with done with t...  \n",
       "26  RT Dat was een beste klim Maar het uitzicht ma...  \n",
       "27  Tafelberg sale shows DA doesnt occupy moral hi...  \n",
       "28  Impossible not to interpret actions and langua...  \n",
       "29             RT Cissie Gool House this morning _�ɍ   "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Cleansing(data)\n",
    "data.cleanup(TwitterCleanuper())\n",
    "data.processed_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "class TwitterData_TokenStem(TwitterData_Cleansing):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "    emoticons_str = r\"\"\"\n",
    "        (?:\n",
    "            [:=;] # Eyes\n",
    "            [oO\\-]? # Nose (optional)\n",
    "            [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "        )\"\"\"\n",
    "\n",
    "    regex_str = [\n",
    "        emoticons_str,\n",
    "        r'<[^>]+>', # HTML tags\n",
    "        r'(?:@[\\w_]+)', # @-mentions\n",
    "        r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "        r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "        r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "        r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "        r'(?:[\\w_]+)', # other words\n",
    "        r'(?:\\S)' # anything else\n",
    "    ]\n",
    "\n",
    "    tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "    emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "    \n",
    "\n",
    "    def stem(self, stemmer=nltk.PorterStemmer()):\n",
    "        def stem_and_join(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "\n",
    "    def tokenize(self, tokenizer=nltk.word_tokenize):\n",
    "        def tokenize_row(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenized_text\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "\n",
    "        self.processed_data = self.processed_data.apply(tokenize_row, axis=1)\n",
    "\n",
    "    def preprocess(s, lowercase=False):\n",
    "        tokens = tokenize(processed_data)\n",
    "        if lowercase:\n",
    "            tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('expected string or buffer', u'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-650b75294ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitterData_TokenStem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#data.stem()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-ecfb677a22d8>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, tokenizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4260\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4262\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4357\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4358\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4359\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4360\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-ecfb677a22d8>\u001b[0m in \u001b[0;36mtokenize_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \"\"\"\n\u001b[1;32m   1313\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boratonaj/anaconda2/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('expected string or buffer', u'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "data = TwitterData_TokenStem(data)\n",
    "data.tokenize()\n",
    "#data.stem()\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 8344),\n",
       " ('e', 4604),\n",
       " ('a', 3472),\n",
       " ('t', 3419),\n",
       " ('i', 2928),\n",
       " ('o', 2768),\n",
       " ('s', 2646),\n",
       " ('n', 2550),\n",
       " ('r', 1886),\n",
       " ('l', 1822),\n",
       " ('h', 1722),\n",
       " ('d', 1288),\n",
       " ('c', 1222),\n",
       " ('u', 1112),\n",
       " ('p', 1101),\n",
       " ('m', 920),\n",
       " ('g', 903),\n",
       " ('T', 801),\n",
       " ('_', 721),\n",
       " ('f', 654),\n",
       " ('y', 628),\n",
       " ('w', 606),\n",
       " ('R', 570),\n",
       " ('b', 549),\n",
       " ('\\xd1', 348),\n",
       " ('v', 321),\n",
       " ('k', 273),\n",
       " ('S', 228),\n",
       " ('A', 215),\n",
       " ('C', 206),\n",
       " ('W', 168),\n",
       " ('I', 136),\n",
       " ('P', 118),\n",
       " ('Z', 96),\n",
       " ('E', 95),\n",
       " ('B', 93),\n",
       " ('F', 82),\n",
       " ('G', 78),\n",
       " ('M', 77),\n",
       " ('H', 73),\n",
       " ('\\xaa', 69),\n",
       " ('K', 68),\n",
       " ('\\xe9', 66),\n",
       " ('x', 63),\n",
       " ('U', 62),\n",
       " ('\\r', 57),\n",
       " ('L', 57),\n",
       " ('j', 56),\n",
       " ('D', 54),\n",
       " ('N', 53)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_words():\n",
    "    words = Counter()\n",
    "    for idx in data.processed_data.index:\n",
    "        words.update(data.processed_data.loc[idx, \"text\"])\n",
    "    return words.most_common(50)\n",
    "most_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The most commont words (as expected) are the typical english stopwords. We will filter them out, however, as purpose of this analysis is to determine sentiment, words like \"cape town\" and \"south africa\" can influence it greatly. Having this in mind, this word will be whitelisted.Try calling read_csv with encoding='latin1', encoding='iso-8859-1' or encoding='cp1252'; these the various encodings found on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f5c1c0cce413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstop_word\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwhitelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"Cape Town\", \"South Africa\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TwitterData_TokenStem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f67acd4b9ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTwitterData_Wordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTwitterData_TokenStem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwhitelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Cape Town\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"South Africa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TwitterData_TokenStem' is not defined"
     ]
    }
   ],
   "source": [
    "class TwitterData_Wordlist(TwitterData_TokenStem):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        \n",
    "    whitelist = [\"Cape Town\", \"South Africa\"]\n",
    "    wordlist = []\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=3, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile(\"/Users/boratonaj/Desktop/Data_Science/Assessment/CT_Tweets_Analysis_Report/data/wordlist.csv\"):\n",
    "            word_df = pd.read_csv(\"/Users/boratonaj/Desktop/Data_Science/Assessment/CT_Tweets_Analysis_Report/data/wordlist.csv\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "\n",
    "        words = Counter()\n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"text\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"/Users/boratonaj/Desktop/Data_Science/Assessment/CT_Tweets_Analysis_Report/data/wordlist.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = TwitterData_Wordlist(data)\n",
    "data.build_wordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "x": [
          41,
          41,
          43,
          44,
          45,
          47,
          48,
          50,
          57,
          118,
          472
         ],
         "y": [
          "spatial",
          "meeting",
          "movement",
          "https_Ñ_",
          "apartheid",
          "The",
          "Cape",
          "Khayelitsha",
          "amp",
          "Tafelberg",
          "RT"
         ]
        }
       ],
       "layout": {
        "title": "Top words in built wordlist"
       }
      },
      "text/html": [
       "<div id=\"d46772c3-c8e3-47dd-8c36-b083d9e86701\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d46772c3-c8e3-47dd-8c36-b083d9e86701\", [{\"y\": [\"spatial\", \"meeting\", \"movement\", \"https_\\u00d1_\", \"apartheid\", \"The\", \"Cape\", \"Khayelitsha\", \"amp\", \"Tafelberg\", \"RT\"], \"x\": [41, 41, 43, 44, 45, 47, 48, 50, 57, 118, 472], \"type\": \"bar\", \"orientation\": \"h\"}], {\"title\": \"Top words in built wordlist\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d46772c3-c8e3-47dd-8c36-b083d9e86701\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d46772c3-c8e3-47dd-8c36-b083d9e86701\", [{\"y\": [\"spatial\", \"meeting\", \"movement\", \"https_\\u00d1_\", \"apartheid\", \"The\", \"Cape\", \"Khayelitsha\", \"amp\", \"Tafelberg\", \"RT\"], \"x\": [41, 41, 43, 44, 45, 47, 48, 50, 57, 118, 472], \"type\": \"bar\", \"orientation\": \"h\"}], {\"title\": \"Top words in built wordlist\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = pd.read_csv(\"/Users/boratonaj/Desktop/Data_Science/Assessment/CT_Tweets_Analysis_Report/data/wordlist.csv\",encoding='latin1')\n",
    "x_words = list(words.loc[0:10,\"word\"])\n",
    "x_words.reverse()\n",
    "y_occ = list(words.loc[0:10,\"occurrences\"])\n",
    "y_occ.reverse()\n",
    "\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x=y_occ,\n",
    "        y=x_words,\n",
    "        orientation=\"h\"\n",
    ")]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Top words in built wordlist\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_BagOfWords(TwitterData_Wordlist):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_data = previous.processed_data\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        if not self.is_testing:\n",
    "            label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + list(\n",
    "            map(lambda w: w + \"_cpt\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if not self.is_testing:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"location\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>RT_cpt</th>\n",
       "      <th>Tafelberg_cpt</th>\n",
       "      <th>amp_cpt</th>\n",
       "      <th>Khayelitsha_cpt</th>\n",
       "      <th>Cape_cpt</th>\n",
       "      <th>The_cpt</th>\n",
       "      <th>apartheid_cpt</th>\n",
       "      <th>https_�__cpt</th>\n",
       "      <th>movement_cpt</th>\n",
       "      <th>...</th>\n",
       "      <th>rising_cpt</th>\n",
       "      <th>inner_cpt</th>\n",
       "      <th>raising_cpt</th>\n",
       "      <th>Africa_cpt</th>\n",
       "      <th>ur_cpt</th>\n",
       "      <th>agenda_cpt</th>\n",
       "      <th>fabricating_cpt</th>\n",
       "      <th>deserve_cpt</th>\n",
       "      <th>daily_cpt</th>\n",
       "      <th>push_cpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cape town</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 473 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label  RT_cpt  Tafelberg_cpt  amp_cpt  Khayelitsha_cpt  Cape_cpt  \\\n",
       "0  South Africa       1              0        0                0         0   \n",
       "1  South Africa       1              0        0                0         0   \n",
       "2  South Africa       1              1        0                0         0   \n",
       "3  South Africa       1              1        0                0         0   \n",
       "4     cape town       0              0        0                0         0   \n",
       "\n",
       "   The_cpt  apartheid_cpt  https_�__cpt  movement_cpt    ...     rising_cpt  \\\n",
       "0        0              0             0             0    ...              0   \n",
       "1        0              0             0             0    ...              0   \n",
       "2        0              0             1             0    ...              0   \n",
       "3        0              0             1             0    ...              0   \n",
       "4        1              0             0             0    ...              0   \n",
       "\n",
       "   inner_cpt  raising_cpt  Africa_cpt  ur_cpt  agenda_cpt  fabricating_cpt  \\\n",
       "0          0            0           0       0           0                0   \n",
       "1          0            0           0       0           0                0   \n",
       "2          0            0           0       0           0                0   \n",
       "3          0            0           0       0           0                0   \n",
       "4          0            0           0       0           0                0   \n",
       "\n",
       "   deserve_cpt  daily_cpt  push_cpt  \n",
       "0            0          0         0  \n",
       "1            0          0         0  \n",
       "2            0          0         0  \n",
       "3            0          0         0  \n",
       "4            0          0         0  \n",
       "\n",
       "[5 rows x 473 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_BagOfWords(data)\n",
    "bow, labels = data.build_data_model()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "South Africa",
         "type": "bar",
         "x": [
          "RT",
          "UniteBehind",
          "movement",
          "Tafelberg",
          "apartheid",
          "The",
          "things",
          "We",
          "spatial",
          "A",
          "done",
          "occupied",
          "prime",
          "amp",
          "land",
          "state",
          "pieces"
         ],
         "y": [
          98,
          17,
          17,
          16,
          16,
          15,
          14,
          6,
          10,
          5,
          8,
          1,
          0,
          11,
          1,
          0,
          0
         ]
        },
        {
         "name": "Cape Town, South Africa",
         "type": "bar",
         "x": [
          "RT",
          "UniteBehind",
          "movement",
          "Tafelberg",
          "apartheid",
          "The",
          "things",
          "We",
          "spatial",
          "A",
          "done",
          "occupied",
          "prime",
          "amp",
          "land",
          "state",
          "pieces"
         ],
         "y": [
          81,
          2,
          4,
          16,
          11,
          7,
          4,
          12,
          11,
          10,
          10,
          4,
          0,
          3,
          3,
          0,
          0
         ]
        },
        {
         "name": "Cape Town",
         "type": "bar",
         "x": [
          "RT",
          "UniteBehind",
          "movement",
          "Tafelberg",
          "apartheid",
          "The",
          "things",
          "We",
          "spatial",
          "A",
          "done",
          "occupied",
          "prime",
          "amp",
          "land",
          "state",
          "pieces"
         ],
         "y": [
          26,
          0,
          8,
          12,
          0,
          6,
          0,
          0,
          2,
          0,
          0,
          16,
          16,
          15,
          15,
          14,
          14
         ]
        }
       ],
       "layout": {
        "title": "Most common words across sentiments"
       }
      },
      "text/html": [
       "<div id=\"29b3dcaa-84de-4f74-80e4-28a610d0d384\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"29b3dcaa-84de-4f74-80e4-28a610d0d384\", [{\"y\": [98, 17, 17, 16, 16, 15, 14, 6, 10, 5, 8, 1, 0, 11, 1, 0, 0], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"South Africa\"}, {\"y\": [81, 2, 4, 16, 11, 7, 4, 12, 11, 10, 10, 4, 0, 3, 3, 0, 0], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"Cape Town, South Africa\"}, {\"y\": [26, 0, 8, 12, 0, 6, 0, 0, 2, 0, 0, 16, 16, 15, 15, 14, 14], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"Cape Town\"}], {\"title\": \"Most common words across sentiments\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"29b3dcaa-84de-4f74-80e4-28a610d0d384\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"29b3dcaa-84de-4f74-80e4-28a610d0d384\", [{\"y\": [98, 17, 17, 16, 16, 15, 14, 6, 10, 5, 8, 1, 0, 11, 1, 0, 0], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"South Africa\"}, {\"y\": [81, 2, 4, 16, 11, 7, 4, 12, 11, 10, 10, 4, 0, 3, 3, 0, 0], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"Cape Town, South Africa\"}, {\"y\": [26, 0, 8, 12, 0, 6, 0, 0, 2, 0, 0, 16, 16, 15, 15, 14, 14], \"x\": [\"RT\", \"UniteBehind\", \"movement\", \"Tafelberg\", \"apartheid\", \"The\", \"things\", \"We\", \"spatial\", \"A\", \"done\", \"occupied\", \"prime\", \"amp\", \"land\", \"state\", \"pieces\"], \"type\": \"bar\", \"name\": \"Cape Town\"}], {\"title\": \"Most common words across sentiments\"}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "words_to_visualize = []\n",
    "sentiments = [\"South Africa\",\"Cape Town, South Africa\",\"Cape Town\"]\n",
    "#get the most 7 common words for every sentiment\n",
    "for sentiment in sentiments:\n",
    "    words = grouped.loc[sentiment,:]\n",
    "    words.sort_values(inplace=True,ascending=False)\n",
    "    for w in words.index[:7]:\n",
    "        if w not in words_to_visualize:\n",
    "            words_to_visualize.append(w)\n",
    "            \n",
    "            \n",
    "#visualize it\n",
    "plot_data = []\n",
    "for sentiment in sentiments:\n",
    "    plot_data.append(graph_objs.Bar(\n",
    "            x = [w.split(\"_\")[0] for w in words_to_visualize],\n",
    "            y = [grouped.loc[sentiment,w] for w in words_to_visualize],\n",
    "            name = sentiment\n",
    "    ))\n",
    "    \n",
    "plotly.offline.iplot({\n",
    "        \"data\":plot_data,\n",
    "        \"layout\":graph_objs.Layout(title=\"Most common words across sentiments\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
